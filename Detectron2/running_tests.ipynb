{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1710606741750,"user":{"displayName":"Mohammed El-dor","userId":"10177968864037735714"},"user_tz":-60},"id":"e8titc9sGKEt","outputId":"a97d56d4-d4e5-4e3b-d3f9-78e8ed41fb78"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Thesis_Organized\n"]}],"source":["cd /content/drive/MyDrive/Thesis_Organized"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tgPa9v-OHGkL"},"outputs":[],"source":["!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":275,"status":"ok","timestamp":1710607041788,"user":{"displayName":"Mohammed El-dor","userId":"10177968864037735714"},"user_tz":-60},"id":"UesCMdsjG27g","outputId":"386021d5-15fa-43ed-f6aa-a096269cf42b"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/Thesis_Organized'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","from tqdm import tqdm\n","main_dir = os.getcwd()\n","main_dir"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2345,"status":"ok","timestamp":1710607045496,"user":{"displayName":"Mohammed El-dor","userId":"10177968864037735714"},"user_tz":-60},"id":"A47hT5s5HCzG","outputId":"8d30ffa5-c24e-4b8f-efca-81d0334b4d3c"},"outputs":[{"name":"stdout","output_type":"stream","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2023 NVIDIA Corporation\n","Built on Tue_Aug_15_22:02:13_PDT_2023\n","Cuda compilation tools, release 12.2, V12.2.140\n","Build cuda_12.2.r12.2/compiler.33191640_0\n","torch:  2.2 ; cuda:  cu121\n","detectron2: 0.6\n"]}],"source":["# Setups\n","import torch, detectron2\n","!nvcc --version\n","TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n","CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n","print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n","print(\"detectron2:\", detectron2.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O7ZnV4ZhIAdE"},"outputs":[],"source":["import detectron2\n","# from detectron2.utils.logger import setup_logger\n","# setup_logger()\n","\n","# import some common libraries\n","import numpy as np\n","import cv2\n","import random\n","from google.colab.patches import cv2_imshow\n","\n","# import some common detectron2 utilities\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":265,"status":"ok","timestamp":1710608317550,"user":{"displayName":"Mohammed El-dor","userId":"10177968864037735714"},"user_tz":-60},"id":"ecEqiTYvIYWd","outputId":"478700a0-8240-4a96-9222-4cd219657ea9"},"outputs":[{"data":{"text/plain":["(0.9, 0.00016)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# importing\n","from Detectron2.detectron_conf import *\n","Detectron2_cfg[\"Momentum\"], Detectron2_cfg[\"base_lr\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XT4Pp8m9IDC_"},"outputs":[],"source":["from detectron2.data.datasets import register_coco_instances\n","register_coco_instances(\"my_dataset_train\", {}, Data_cfg[\"Coco_labels_train_dir\"], os.path.join(Data_cfg[\"cropped_Images_dir\"], \"Train\"))\n","register_coco_instances(\"my_dataset_test\", {}, Data_cfg[\"Coco_labels_test_dir\"], os.path.join(Data_cfg[\"cropped_Images_dir\"], \"Test\"))\n","register_coco_instances(\"my_dataset_val\", {}, Data_cfg[\"Coco_labels_val_dir\"], os.path.join(Data_cfg[\"cropped_Images_dir\"], \"Val\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SR4kNY0DIQh2"},"outputs":[],"source":["from detectron2.engine.hooks import HookBase\n","from detectron2.evaluation import inference_context\n","from detectron2.utils.logger import log_every_n_seconds\n","from detectron2.data import DatasetMapper, build_detection_test_loader\n","import detectron2.utils.comm as comm\n","import torch\n","import time\n","import datetime\n","import logging\n","import numpy as np  # Ensure numpy is imported for mean calculation\n","\n","class LossEvalHook(HookBase):\n","    \"\"\"\n","    A custom hook for periodically evaluating the loss on a validation dataset during training.\n","\n","    This hook leverages Detectron2's HookBase to integrate loss evaluation directly into the training loop.\n","    It computes the loss for each batch in the provided data loader and calculates the mean loss over the entire dataset.\n","    This mean loss is then logged and stored for monitoring the model's performance on unseen data during training.\n","\n","    Attributes:\n","        _model (torch.nn.Module): The model being trained and evaluated.\n","        _period (int): The evaluation period, i.e., how often (in terms of training iterations) to evaluate.\n","        _data_loader (iterable): The data loader for the validation dataset.\n","    \"\"\"\n","\n","    def __init__(self, eval_period, model, data_loader):\n","        \"\"\"\n","        Initializes the LossEvalHook.\n","\n","        Args:\n","            eval_period (int): The number of training iterations between each evaluation.\n","            model (torch.nn.Module): The model that will be evaluated.\n","            data_loader (iterable): The DataLoader providing the validation dataset.\n","        \"\"\"\n","        self._model = model\n","        self._period = eval_period\n","        self._data_loader = data_loader\n","\n","    def _do_loss_eval(self):\n","        \"\"\"\n","        Perform the loss evaluation on the validation dataset.\n","\n","        Iterates over the validation dataset, computes the loss for each batch, and calculates the mean loss.\n","        This function also handles logging progress and synchronization in distributed training setups.\n","\n","        Returns:\n","            List of loss values for each batch in the validation dataset.\n","        \"\"\"\n","        total = len(self._data_loader)  # Total number of batches\n","        num_warmup = min(5, total - 1)  # Number of batches to skip for warm-up\n","\n","        start_time = time.perf_counter()\n","        total_compute_time = 0\n","        losses = []  # Store losses for each batch\n","        for idx, inputs in enumerate(self._data_loader):\n","            # Reset timing and loss calculation after warm-up period\n","            if idx == num_warmup:\n","                start_time = time.perf_counter()\n","                total_compute_time = 0\n","            start_compute_time = time.perf_counter()\n","            if torch.cuda.is_available():\n","                torch.cuda.synchronize()  # Ensure synchronization in CUDA operations\n","            total_compute_time += time.perf_counter() - start_compute_time\n","            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n","            seconds_per_img = total_compute_time / iters_after_start\n","            # Log progress and ETA after warm-up period\n","            if idx >= num_warmup * 2 or seconds_per_img > 5:\n","                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start\n","                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))\n","                log_every_n_seconds(\n","                    logging.INFO,\n","                    f\"Loss on Validation  done {idx + 1}/{total}. {seconds_per_img:.4f} s / img. ETA={eta}\",\n","                    n=5,\n","                )\n","            loss_batch = self._get_loss(inputs)  # Compute loss for the current batch\n","            losses.append(loss_batch)\n","        mean_loss = np.mean(losses)  # Calculate mean loss\n","        self.trainer.storage.put_scalar('validation_loss', mean_loss)\n","        comm.synchronize()  # Synchronize across all processes\n","\n","        return losses\n","\n","    def _get_loss(self, data):\n","        \"\"\"\n","        Calculate and return the loss for a batch of data.\n","\n","        This method forwards the data through the model and aggregates the loss values.\n","\n","        Args:\n","            data (dict): A batch of data to be processed by the model.\n","\n","        Returns:\n","            float: The total loss for the batch.\n","        \"\"\"\n","        metrics_dict = self._model(data)\n","        # Ensure all metrics are scalars and detach any tensors from the graph\n","        metrics_dict = {\n","            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n","            for k, v in metrics_dict.items()\n","        }\n","        total_losses_reduced = sum(loss for loss in metrics_dict.values())  # Sum up the losses\n","        return total_losses_reduced\n","\n","    def after_step(self):\n","        next_iter = self.trainer.iter + 1\n","        is_final = next_iter == self.trainer.max_iter\n","        if is_final or (self._period > 0 and next_iter % self._period == 0):\n","            self._do_loss_eval()\n","        self.trainer.storage.put_scalars(timetest=12)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rOJnGF0rQs1_"},"outputs":[],"source":["from detectron2.data import DatasetMapper, build_detection_test_loader, build_detection_train_loader\n","from detectron2.engine import DefaultTrainer\n","from detectron2.data.transforms import RandomApply, RandomBrightness, RandomRotation, RandomFlip, RandomCrop, RandomContrast\n","from detectron2.config import CfgNode\n","from detectron2.solver.build import get_default_optimizer_params, maybe_add_gradient_clipping\n","import torch\n","\n","class CustomTrainer(DefaultTrainer):\n","    \"\"\"\n","    This class extends Detectron2's DefaultTrainer to include custom behavior for the training process.\n","    It allows for the addition of a loss evaluation hook to periodically assess the model's performance\n","    on a validation or test set during training. Moreover, it customizes the data loading with specific\n","    data augmentations and utilizes a custom optimizer configuration.\n","    \"\"\"\n","\n","    def build_hooks(self):\n","        \"\"\"\n","        Overrides the DefaultTrainer's build_hooks method to insert a custom hook for evaluating\n","        the loss on a validation or test set during the training process. This enables monitoring\n","        the model's performance beyond the training set, providing insights into its generalization capabilities.\n","\n","        Returns:\n","            List[HookBase]: A list of hooks including the custom LossEvalHook for periodic loss evaluation.\n","        \"\"\"\n","        # First, call the parent class's build_hooks method to get the default set of hooks.\n","        hooks = super().build_hooks()\n","\n","        # Insert the custom LossEvalHook before the last hook.\n","        # This ensures that the loss evaluation is performed at the specified intervals.\n","        hooks.insert(-1, LossEvalHook(\n","            eval_period=20,  # Specifies the interval (in terms of training iterations) for performing loss evaluation.\n","            model=self.model,  # Passes the current model for evaluation.\n","            data_loader=build_detection_test_loader(\n","                self.cfg,\n","                self.cfg.DATASETS.TEST[0],  # Specifies the dataset used for evaluation.\n","                DatasetMapper(self.cfg, is_train=True)  # Uses the DatasetMapper with the current configuration.\n","            )\n","        ))\n","\n","        return hooks\n","\n","    @classmethod\n","    def build_train_loader(cls, cfg):\n","        \"\"\"\n","        Customizes the training DataLoader by specifying data augmentations that are applied\n","        to the training dataset. This method enhances the model's ability to generalize by introducing\n","        variability into the training data.\n","\n","        Args:\n","            cfg (CfgNode): Configuration node containing settings for data loading and augmentations.\n","\n","        Returns:\n","            DataLoader: A DataLoader for training, configured with custom data augmentations.\n","        \"\"\"\n","        # Defines a mapper that applies a series of data augmentations to each training example.\n","        mapper = DatasetMapper(cfg, is_train=True, augmentations=[\n","            # Randomly applies brightness adjustment with the specified probability.\n","            RandomApply(RandomBrightness(*Augmentation_cfg[\"RandomBrightness\"][:-1]), Augmentation_cfg[\"RandomBrightness\"][-1]),\n","            # Randomly applies rotation with the specified probability.\n","            RandomApply(RandomRotation(angle=Augmentation_cfg[\"RandomRotation\"][:-1]), Augmentation_cfg[\"RandomRotation\"][-1]),\n","            # Randomly applies horizontal flip with the specified probability.\n","            RandomApply(RandomFlip(), Augmentation_cfg[\"RandomFlip\"][0]),\n","            # Randomly applies cropping with the specified probability.\n","            RandomApply(RandomCrop(\"relative\", Augmentation_cfg[\"RandomCrop\"][:-1]), Augmentation_cfg[\"RandomCrop\"][-1]),\n","            # Randomly applies contrast adjustment with the specified probability.\n","            RandomApply(RandomContrast(*Augmentation_cfg[\"RandomContrast\"][:-1]), Augmentation_cfg[\"RandomContrast\"][-1])\n","        ])\n","\n","        # Builds and returns a DataLoader using the defined mapper for data augmentation.\n","        return build_detection_train_loader(cfg, mapper=mapper)\n","\n","    @classmethod\n","    def build_optimizer(cls, cfg: CfgNode, model: torch.nn.Module) -> torch.optim.Optimizer:\n","        \"\"\"\n","        Configures and returns a custom optimizer for the model, using the AdamW algorithm.\n","        This method allows for custom settings for the optimizer, including learning rates, weight decay, and possibly\n","        gradient clipping, based on the provided configuration.\n","\n","        Args:\n","            cfg (CfgNode): Configuration node containing optimizer settings.\n","            model (torch.nn.Module): The model for which the optimizer is being configured.\n","\n","        Returns:\n","            torch.optim.Optimizer: An instance of the AdamW optimizer with configured parameters.\n","        \"\"\"\n","        # Retrieves default parameters for optimizer setup, including learning rate and weight decay adjustments.\n","        params = get_default_optimizer_params(\n","            model,\n","            base_lr=cfg.SOLVER.BASE_LR,\n","            weight_decay_norm=cfg.SOLVER.WEIGHT_DECAY_NORM,\n","            bias_lr_factor=cfg.SOLVER.BIAS_LR_FACTOR,\n","            weight_decay=cfg.SOLVER.WEIGHT_DECAY,\n","        )\n","        return maybe_add_gradient_clipping(cfg, torch.optim.AdamW)(\n","            params,\n","            lr=cfg.SOLVER.BASE_LR,\n","            weight_decay=cfg.SOLVER.WEIGHT_DECAY,\n","        )"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"0FGk14CySBZv","executionInfo":{"status":"ok","timestamp":1711558225135,"user_tz":-60,"elapsed":349,"user":{"displayName":"Mohammed El-dor","userId":"10177968864037735714"}}},"outputs":[],"source":["from detectron2.engine import DefaultTrainer\n","from detectron2.config import get_cfg\n","cfg = get_cfg()"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1711558225380,"user":{"displayName":"Mohammed El-dor","userId":"10177968864037735714"},"user_tz":-60},"id":"SQAf7cPfSM6T","outputId":"1511fdfd-c064-4ef6-b3cf-f95227481258"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CfgNode({'VERSION': 2, 'MODEL': CfgNode({'LOAD_PROPOSALS': False, 'MASK_ON': False, 'KEYPOINT_ON': False, 'DEVICE': 'cuda', 'META_ARCHITECTURE': 'GeneralizedRCNN', 'WEIGHTS': '', 'PIXEL_MEAN': [103.53, 116.28, 123.675], 'PIXEL_STD': [1.0, 1.0, 1.0], 'BACKBONE': CfgNode({'NAME': 'build_resnet_backbone', 'FREEZE_AT': 2}), 'FPN': CfgNode({'IN_FEATURES': [], 'OUT_CHANNELS': 256, 'NORM': '', 'FUSE_TYPE': 'sum'}), 'PROPOSAL_GENERATOR': CfgNode({'NAME': 'RPN', 'MIN_SIZE': 0}), 'ANCHOR_GENERATOR': CfgNode({'NAME': 'DefaultAnchorGenerator', 'SIZES': [[32, 64, 128, 256, 512]], 'ASPECT_RATIOS': [[0.5, 1.0, 2.0]], 'ANGLES': [[-90, 0, 90]], 'OFFSET': 0.0}), 'RPN': CfgNode({'HEAD_NAME': 'StandardRPNHead', 'IN_FEATURES': ['res4'], 'BOUNDARY_THRESH': -1, 'IOU_THRESHOLDS': [0.3, 0.7], 'IOU_LABELS': [0, -1, 1], 'BATCH_SIZE_PER_IMAGE': 256, 'POSITIVE_FRACTION': 0.5, 'BBOX_REG_LOSS_TYPE': 'smooth_l1', 'BBOX_REG_LOSS_WEIGHT': 1.0, 'BBOX_REG_WEIGHTS': (1.0, 1.0, 1.0, 1.0), 'SMOOTH_L1_BETA': 0.0, 'LOSS_WEIGHT': 1.0, 'PRE_NMS_TOPK_TRAIN': 12000, 'PRE_NMS_TOPK_TEST': 6000, 'POST_NMS_TOPK_TRAIN': 2000, 'POST_NMS_TOPK_TEST': 1000, 'NMS_THRESH': 0.7, 'CONV_DIMS': [-1]}), 'ROI_HEADS': CfgNode({'NAME': 'Res5ROIHeads', 'NUM_CLASSES': 80, 'IN_FEATURES': ['res4'], 'IOU_THRESHOLDS': [0.5], 'IOU_LABELS': [0, 1], 'BATCH_SIZE_PER_IMAGE': 512, 'POSITIVE_FRACTION': 0.25, 'SCORE_THRESH_TEST': 0.05, 'NMS_THRESH_TEST': 0.5, 'PROPOSAL_APPEND_GT': True}), 'ROI_BOX_HEAD': CfgNode({'NAME': '', 'BBOX_REG_LOSS_TYPE': 'smooth_l1', 'BBOX_REG_LOSS_WEIGHT': 1.0, 'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0), 'SMOOTH_L1_BETA': 0.0, 'POOLER_RESOLUTION': 14, 'POOLER_SAMPLING_RATIO': 0, 'POOLER_TYPE': 'ROIAlignV2', 'NUM_FC': 0, 'FC_DIM': 1024, 'NUM_CONV': 0, 'CONV_DIM': 256, 'NORM': '', 'CLS_AGNOSTIC_BBOX_REG': False, 'TRAIN_ON_PRED_BOXES': False, 'USE_FED_LOSS': False, 'USE_SIGMOID_CE': False, 'FED_LOSS_FREQ_WEIGHT_POWER': 0.5, 'FED_LOSS_NUM_CLASSES': 50}), 'ROI_BOX_CASCADE_HEAD': CfgNode({'BBOX_REG_WEIGHTS': ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0)), 'IOUS': (0.5, 0.6, 0.7)}), 'ROI_MASK_HEAD': CfgNode({'NAME': 'MaskRCNNConvUpsampleHead', 'POOLER_RESOLUTION': 14, 'POOLER_SAMPLING_RATIO': 0, 'NUM_CONV': 0, 'CONV_DIM': 256, 'NORM': '', 'CLS_AGNOSTIC_MASK': False, 'POOLER_TYPE': 'ROIAlignV2'}), 'ROI_KEYPOINT_HEAD': CfgNode({'NAME': 'KRCNNConvDeconvUpsampleHead', 'POOLER_RESOLUTION': 14, 'POOLER_SAMPLING_RATIO': 0, 'CONV_DIMS': (512, 512, 512, 512, 512, 512, 512, 512), 'NUM_KEYPOINTS': 17, 'MIN_KEYPOINTS_PER_IMAGE': 1, 'NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS': True, 'LOSS_WEIGHT': 1.0, 'POOLER_TYPE': 'ROIAlignV2'}), 'SEM_SEG_HEAD': CfgNode({'NAME': 'SemSegFPNHead', 'IN_FEATURES': ['p2', 'p3', 'p4', 'p5'], 'IGNORE_VALUE': 255, 'NUM_CLASSES': 54, 'CONVS_DIM': 128, 'COMMON_STRIDE': 4, 'NORM': 'GN', 'LOSS_WEIGHT': 1.0}), 'PANOPTIC_FPN': CfgNode({'INSTANCE_LOSS_WEIGHT': 1.0, 'COMBINE': CfgNode({'ENABLED': True, 'OVERLAP_THRESH': 0.5, 'STUFF_AREA_LIMIT': 4096, 'INSTANCES_CONFIDENCE_THRESH': 0.5})}), 'RETINANET': CfgNode({'NUM_CLASSES': 80, 'IN_FEATURES': ['p3', 'p4', 'p5', 'p6', 'p7'], 'NUM_CONVS': 4, 'IOU_THRESHOLDS': [0.4, 0.5], 'IOU_LABELS': [0, -1, 1], 'PRIOR_PROB': 0.01, 'SCORE_THRESH_TEST': 0.05, 'TOPK_CANDIDATES_TEST': 1000, 'NMS_THRESH_TEST': 0.5, 'BBOX_REG_WEIGHTS': (1.0, 1.0, 1.0, 1.0), 'FOCAL_LOSS_GAMMA': 2.0, 'FOCAL_LOSS_ALPHA': 0.25, 'SMOOTH_L1_LOSS_BETA': 0.1, 'BBOX_REG_LOSS_TYPE': 'smooth_l1', 'NORM': ''}), 'RESNETS': CfgNode({'DEPTH': 50, 'OUT_FEATURES': ['res4'], 'NUM_GROUPS': 1, 'NORM': 'FrozenBN', 'WIDTH_PER_GROUP': 64, 'STRIDE_IN_1X1': True, 'RES5_DILATION': 1, 'RES2_OUT_CHANNELS': 256, 'STEM_OUT_CHANNELS': 64, 'DEFORM_ON_PER_STAGE': [False, False, False, False], 'DEFORM_MODULATED': False, 'DEFORM_NUM_GROUPS': 1})}), 'INPUT': CfgNode({'MIN_SIZE_TRAIN': (800,), 'MIN_SIZE_TRAIN_SAMPLING': 'choice', 'MAX_SIZE_TRAIN': 1333, 'MIN_SIZE_TEST': 800, 'MAX_SIZE_TEST': 1333, 'RANDOM_FLIP': 'horizontal', 'CROP': CfgNode({'ENABLED': False, 'TYPE': 'relative_range', 'SIZE': [0.9, 0.9]}), 'FORMAT': 'BGR', 'MASK_FORMAT': 'polygon'}), 'DATASETS': CfgNode({'TRAIN': (), 'PROPOSAL_FILES_TRAIN': (), 'PRECOMPUTED_PROPOSAL_TOPK_TRAIN': 2000, 'TEST': (), 'PROPOSAL_FILES_TEST': (), 'PRECOMPUTED_PROPOSAL_TOPK_TEST': 1000}), 'DATALOADER': CfgNode({'NUM_WORKERS': 4, 'ASPECT_RATIO_GROUPING': True, 'SAMPLER_TRAIN': 'TrainingSampler', 'REPEAT_THRESHOLD': 0.0, 'FILTER_EMPTY_ANNOTATIONS': True}), 'SOLVER': CfgNode({'LR_SCHEDULER_NAME': 'WarmupMultiStepLR', 'MAX_ITER': 40000, 'BASE_LR': 0.001, 'BASE_LR_END': 0.0, 'MOMENTUM': 0.9, 'NESTEROV': False, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_NORM': 0.0, 'GAMMA': 0.1, 'STEPS': (30000,), 'NUM_DECAYS': 3, 'WARMUP_FACTOR': 0.001, 'WARMUP_ITERS': 1000, 'WARMUP_METHOD': 'linear', 'RESCALE_INTERVAL': False, 'CHECKPOINT_PERIOD': 5000, 'IMS_PER_BATCH': 16, 'REFERENCE_WORLD_SIZE': 0, 'BIAS_LR_FACTOR': 1.0, 'WEIGHT_DECAY_BIAS': None, 'CLIP_GRADIENTS': CfgNode({'ENABLED': False, 'CLIP_TYPE': 'value', 'CLIP_VALUE': 1.0, 'NORM_TYPE': 2.0}), 'AMP': CfgNode({'ENABLED': False})}), 'TEST': CfgNode({'EXPECTED_RESULTS': [], 'EVAL_PERIOD': 0, 'KEYPOINT_OKS_SIGMAS': [], 'DETECTIONS_PER_IMAGE': 100, 'AUG': CfgNode({'ENABLED': False, 'MIN_SIZES': (400, 500, 600, 700, 800, 900, 1000, 1100, 1200), 'MAX_SIZE': 4000, 'FLIP': True}), 'PRECISE_BN': CfgNode({'ENABLED': False, 'NUM_ITER': 200})}), 'OUTPUT_DIR': './output', 'SEED': -1, 'CUDNN_BENCHMARK': False, 'VIS_PERIOD': 0, 'GLOBAL': CfgNode({'HACK': 1.0})})"]},"metadata":{},"execution_count":19}],"source":["cfg"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"UMvq1acAqGLz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711558225380,"user_tz":-60,"elapsed":4,"user":{"displayName":"Mohammed El-dor","userId":"10177968864037735714"}},"outputId":"c81a31bb-2d0f-407c-8697-7b5d40b50b2e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["('GeneralizedRCNN', CfgNode({'NAME': 'build_resnet_backbone', 'FREEZE_AT': 2}))"]},"metadata":{},"execution_count":20}],"source":["cfg.MODEL.META_ARCHITECTURE, cfg.MODEL.BACKBONE"]},{"cell_type":"code","source":["from detectron2 import model_zoo\n","\n","cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\"))"],"metadata":{"id":"HPK739GEz2o_","executionInfo":{"status":"ok","timestamp":1711558225894,"user_tz":-60,"elapsed":1,"user":{"displayName":"Mohammed El-dor","userId":"10177968864037735714"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["cfg.MODEL.META_ARCHITECTURE, cfg.MODEL.BACKBONE"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J4eS6QJ1z6Uq","executionInfo":{"status":"ok","timestamp":1711558226138,"user_tz":-60,"elapsed":3,"user":{"displayName":"Mohammed El-dor","userId":"10177968864037735714"}},"outputId":"2d9bc696-20d4-4b56-83d8-6165b03fbf14"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('GeneralizedRCNN',\n"," CfgNode({'NAME': 'build_resnet_fpn_backbone', 'FREEZE_AT': 2}))"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":303,"status":"ok","timestamp":1710611590617,"user":{"displayName":"Mohammed El-dor","userId":"10177968864037735714"},"user_tz":-60},"id":"dhqHJ4dlZOBx","outputId":"f7d68115-5d66-4e90-9a23-d98c07daa543"},"outputs":[{"data":{"text/plain":["CfgNode({'NAME': 'build_resnet_backbone', 'FREEZE_AT': 0})"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["cfg[\"MODEL\"]['BACKBONE'] = CfgNode({'NAME': 'build_resnet_backbone', 'FREEZE_AT': 0})\n","cfg.OUTPUT_DIR = \"./Detectron2/Test_results/SGD0.99_001_00_3x101\"\n","cfg.merge_from_file(model_zoo.get_config_file(MODELS_LIST[1]))\n","cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n","cfg.DATASETS.TEST = (\"my_dataset_test\",)\n","cfg.SOLVER.MOMENTUM = 0.99 #Detectron2_cfg[\"Momentum\"]\n","cfg.SOLVER.CHECKPOINT_PERIOD = 200  # The network takes a checkpoint once it finishes of every 200 iterations\n","# cfg.TEST.EVAL_PERIOD = 20\n","cfg.DATALOADER.NUM_WORKERS = NUM_WORKERS\n","cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(MODELS_LIST[1])  # Let training initialize from model zoo\n","cfg.SOLVER.IMS_PER_BATCH = IMS_PER_BATCH  # This is the real \"batch size\" commonly known to deep learning people\n","cfg.SOLVER.BASE_LR = 0.001 # Detectron2_cfg[\"base_lr\"]  # pick a good LR\n","cfg.SOLVER.MAX_ITER = 1500\n","cfg.SOLVER.STEPS = []        # do not decay learning rate\n","cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = BATCH_SIZE_PER_IMAGE  # The \"RoIHead batch size\". 128 is faster, and good enough for this dataset (default: 512)\n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = NUM_CLASSES  # only has one class (Track). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n","# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n","\n","cfg.MODEL.PIXEL_MEAN = [26.9]\n","cfg.MODEL.PIXEL_STD = [34.4]\n","# for grayscale images\n","# if it gives error just remove it and make mean and std 3 values that are equal [26.9, 26.9, 26.9], [34.4, 34.4, 34.4]\n","cfg.INPUT.FORMAT = \"L\"\n","\n","os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n","# trainer = DefaultTrainer(cfg)\n","trainer = CustomTrainer(cfg)\n","trainer.resume_or_load(resume=True)\n","trainer.train()"]},{"cell_type":"code","source":[],"metadata":{"id":"P1Q1gm91qHJm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yX_HtksdqHHg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Jw0R_5hpqHE2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EROGHkOIqJaF","executionInfo":{"status":"ok","timestamp":1711557223685,"user_tz":-60,"elapsed":20099,"user":{"displayName":"Mohammed El-dor","userId":"10177968864037735714"}},"outputId":"c31ad9e0-6691-4af1-ab56-a205a65b9ab6"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/Thesis_Organized/Detectron2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3y1PL7mpqW59","executionInfo":{"status":"ok","timestamp":1711559458872,"user_tz":-60,"elapsed":291,"user":{"displayName":"Mohammed El-dor","userId":"10177968864037735714"}},"outputId":"73893995-9ba8-41b3-8806-5b04be6a33eb"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Thesis_Organized/Detectron2\n"]}]},{"cell_type":"code","source":["pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"ue4Q_Y9rqfPH","executionInfo":{"status":"ok","timestamp":1711557223685,"user_tz":-60,"elapsed":4,"user":{"displayName":"Mohammed El-dor","userId":"10177968864037735714"}},"outputId":"b9e5ce02-5518-45af-aab4-c7959354549a"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Thesis_Organized/Detectron2'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"],"metadata":{"id":"iHBzl-e8qxFC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711557492759,"user_tz":-60,"elapsed":1108,"user":{"displayName":"Mohammed El-dor","userId":"10177968864037735714"}},"outputId":"35f53f7c-07b8-434e-9db8-06627286c19b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n","Successfully installed antlr4-python3-runtime-4.9.3 black-24.3.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.12.1 portalocker-2.8.2 yacs-0.1.8\n"]}]},{"cell_type":"code","source":["!python train_detectron2.py"],"metadata":{"id":"LKD-gWgkqgUa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711559677462,"user_tz":-60,"elapsed":216332,"user":{"displayName":"Mohammed El-dor","userId":"10177968864037735714"}},"outputId":"b25ef610-6593-483c-f518-cf347696762d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\r  0% 0/100 [00:00<?, ?it/s]dict_values(['COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml', 'giou', 'smooth_l1', 'smooth_l1', 0.95, 0.0001])\n","\u001b[32m[03/27 17:11:10 d2.engine.defaults]: \u001b[0mModel:\n","GeneralizedRCNN(\n","  (backbone): FPN(\n","    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (top_block): LastLevelMaxPool()\n","    (bottom_up): ResNet(\n","      (stem): BasicStem(\n","        (conv1): Conv2d(\n","          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","      )\n","      (res2): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res3): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res4): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (4): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (5): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (6): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (7): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (8): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (9): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (10): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (11): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (12): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (13): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (14): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (15): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (16): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (17): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (18): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (19): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (20): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (21): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (22): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res5): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (proposal_generator): RPN(\n","    (rpn_head): StandardRPNHead(\n","      (conv): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (anchor_generator): DefaultAnchorGenerator(\n","      (cell_anchors): BufferList()\n","    )\n","  )\n","  (roi_heads): StandardROIHeads(\n","    (box_pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n","        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n","        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (box_head): FastRCNNConvFCHead(\n","      (flatten): Flatten(start_dim=1, end_dim=-1)\n","      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc_relu1): ReLU()\n","      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n","      (fc_relu2): ReLU()\n","    )\n","    (box_predictor): FastRCNNOutputLayers(\n","      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n","    )\n","    (mask_pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n","        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n","        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (mask_head): MaskRCNNConvUpsampleHead(\n","      (mask_fcn1): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn2): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn3): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn4): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n","      (deconv_relu): ReLU()\n","      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n",")\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:11:10 d2.data.datasets.coco]: \u001b[0m\n","Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n","\n","\u001b[32m[03/27 17:11:10 d2.data.datasets.coco]: \u001b[0mLoaded 791 images in COCO format from /content/drive/MyDrive/Thesis_Organized/Data/COCO_Format/coco_train_data.json\n","\u001b[32m[03/27 17:11:10 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 791 images left.\n","\u001b[32m[03/27 17:11:10 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n","\u001b[36m|  category  | #instances   |\n","|:----------:|:-------------|\n","|   Track    | 2402         |\n","|            |              |\u001b[0m\n","\u001b[32m[03/27 17:11:10 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n","\u001b[32m[03/27 17:11:10 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n","\u001b[32m[03/27 17:11:10 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n","\u001b[32m[03/27 17:11:10 d2.data.common]: \u001b[0mSerializing 791 elements to byte tensors and concatenating them all ...\n","\u001b[32m[03/27 17:11:11 d2.data.common]: \u001b[0mSerialized dataset takes 4.34 MiB\n","\u001b[32m[03/27 17:11:11 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=4\n","\u001b[32m[03/27 17:11:11 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:11:11 d2.data.datasets.coco]: \u001b[0m\n","Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n","\n","\u001b[32m[03/27 17:11:11 d2.data.datasets.coco]: \u001b[0mLoaded 97 images in COCO format from /content/drive/MyDrive/Thesis_Organized/Data/COCO_Format/coco_val_data.json\n","\u001b[32m[03/27 17:11:11 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n","\u001b[36m|  category  | #instances   |\n","|:----------:|:-------------|\n","|   Track    | 309          |\n","|            |              |\u001b[0m\n","\u001b[32m[03/27 17:11:11 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n","\u001b[32m[03/27 17:11:11 d2.data.common]: \u001b[0mSerializing 97 elements to byte tensors and concatenating them all ...\n","\u001b[32m[03/27 17:11:11 d2.data.common]: \u001b[0mSerialized dataset takes 0.55 MiB\n","\u001b[32m[03/27 17:11:11 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x/138205316/model_final_a3ec72.pkl ...\n","\n","model_final_a3ec72.pkl: 0.00B [00:00, ?B/s]\u001b[A\n","model_final_a3ec72.pkl:   0% 24.6k/254M [00:00<17:15, 245kB/s]\u001b[A\n","model_final_a3ec72.pkl:   2% 4.69M/254M [00:00<00:09, 27.5MB/s]\u001b[A\n","model_final_a3ec72.pkl:   4% 10.8M/254M [00:00<00:06, 40.1MB/s]\u001b[A\n","model_final_a3ec72.pkl:   7% 17.6M/254M [00:00<00:04, 50.9MB/s]\u001b[A\n","model_final_a3ec72.pkl:  10% 26.2M/254M [00:00<00:03, 60.9MB/s]\u001b[A\n","model_final_a3ec72.pkl:  14% 36.5M/254M [00:00<00:02, 74.8MB/s]\u001b[A\n","model_final_a3ec72.pkl:  20% 49.6M/254M [00:00<00:02, 92.5MB/s]\u001b[A\n","model_final_a3ec72.pkl:  26% 64.9M/254M [00:00<00:01, 111MB/s] \u001b[A\n","model_final_a3ec72.pkl:  33% 83.8M/254M [00:00<00:01, 135MB/s]\u001b[A\n","model_final_a3ec72.pkl:  41% 105M/254M [00:01<00:00, 158MB/s] \u001b[A\n","model_final_a3ec72.pkl:  47% 119M/254M [00:01<00:00, 154MB/s]\u001b[A\n","model_final_a3ec72.pkl:  52% 132M/254M [00:01<00:00, 146MB/s]\u001b[A\n","model_final_a3ec72.pkl:  59% 151M/254M [00:01<00:00, 158MB/s]\u001b[A\n","model_final_a3ec72.pkl:  68% 173M/254M [00:01<00:00, 177MB/s]\u001b[A\n","model_final_a3ec72.pkl:  75% 190M/254M [00:01<00:00, 175MB/s]\u001b[A\n","model_final_a3ec72.pkl:  82% 209M/254M [00:01<00:00, 180MB/s]\u001b[A\n","model_final_a3ec72.pkl:  90% 229M/254M [00:01<00:00, 187MB/s]\u001b[A\n","model_final_a3ec72.pkl: 254MB [00:01, 137MB/s]               \n","Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n","Some model parameters or buffers are not found in the checkpoint:\n","\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n","\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n","\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n","\u001b[32m[03/27 17:11:13 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n","/usr/local/lib/python3.10/dist-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","\u001b[32m[03/27 17:11:42 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 9  total_loss: 2.613  loss_cls: 0.6847  loss_box_reg: 0.4879  loss_mask: 0.6835  loss_rpn_cls: 0.6604  loss_rpn_loc: 0.07956  validation_loss: 2.285    time: 1.2882  last_time: 1.2413  data_time: 0.1470  last_data_time: 0.0133   lr: 9.001e-05  max_mem: 6349M\n","2024-03-27 17:11:42.833782: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-03-27 17:11:42.833833: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-03-27 17:11:42.835358: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-03-27 17:11:44.078245: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[32m[03/27 17:11:45 d2.engine.hooks]: \u001b[0mOverall training speed: 8 iterations in 0:00:10 (1.2882 s / it)\n","\u001b[32m[03/27 17:11:45 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:27 (0:00:17 on hooks)\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:11:45 d2.data.datasets.coco]: \u001b[0m\n","Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n","\n","\u001b[32m[03/27 17:11:45 d2.data.datasets.coco]: \u001b[0mLoaded 97 images in COCO format from /content/drive/MyDrive/Thesis_Organized/Data/COCO_Format/coco_val_data.json\n","\u001b[32m[03/27 17:11:45 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n","\u001b[32m[03/27 17:11:45 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n","\u001b[32m[03/27 17:11:45 d2.data.common]: \u001b[0mSerializing 97 elements to byte tensors and concatenating them all ...\n","\u001b[32m[03/27 17:11:45 d2.data.common]: \u001b[0mSerialized dataset takes 0.55 MiB\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:11:45 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n","  1% 1/100 [00:36<59:51, 36.27s/it]dict_values(['COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml', 'smooth_l1', 'ciou', 'giou', 0.9, 0.0002])\n","\u001b[32m[03/27 17:11:46 d2.engine.defaults]: \u001b[0mModel:\n","GeneralizedRCNN(\n","  (backbone): FPN(\n","    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (top_block): LastLevelMaxPool()\n","    (bottom_up): ResNet(\n","      (stem): BasicStem(\n","        (conv1): Conv2d(\n","          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","      )\n","      (res2): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res3): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res4): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (4): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (5): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res5): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (proposal_generator): RPN(\n","    (rpn_head): StandardRPNHead(\n","      (conv): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (anchor_generator): DefaultAnchorGenerator(\n","      (cell_anchors): BufferList()\n","    )\n","  )\n","  (roi_heads): StandardROIHeads(\n","    (box_pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n","        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n","        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (box_head): FastRCNNConvFCHead(\n","      (flatten): Flatten(start_dim=1, end_dim=-1)\n","      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc_relu1): ReLU()\n","      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n","      (fc_relu2): ReLU()\n","    )\n","    (box_predictor): FastRCNNOutputLayers(\n","      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n","    )\n","    (mask_pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n","        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n","        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (mask_head): MaskRCNNConvUpsampleHead(\n","      (mask_fcn1): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn2): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn3): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn4): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n","      (deconv_relu): ReLU()\n","      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n",")\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:11:46 d2.data.datasets.coco]: \u001b[0m\n","Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n","\n","\u001b[32m[03/27 17:11:46 d2.data.datasets.coco]: \u001b[0mLoaded 791 images in COCO format from /content/drive/MyDrive/Thesis_Organized/Data/COCO_Format/coco_train_data.json\n","\u001b[32m[03/27 17:11:46 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 791 images left.\n","\u001b[32m[03/27 17:11:46 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n","\u001b[32m[03/27 17:11:46 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n","\u001b[32m[03/27 17:11:46 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n","\u001b[32m[03/27 17:11:46 d2.data.common]: \u001b[0mSerializing 791 elements to byte tensors and concatenating them all ...\n","\u001b[32m[03/27 17:11:46 d2.data.common]: \u001b[0mSerialized dataset takes 4.34 MiB\n","\u001b[32m[03/27 17:11:46 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=4\n","\u001b[32m[03/27 17:11:46 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:11:46 d2.data.datasets.coco]: \u001b[0m\n","Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n","\n","\u001b[32m[03/27 17:11:46 d2.data.datasets.coco]: \u001b[0mLoaded 97 images in COCO format from /content/drive/MyDrive/Thesis_Organized/Data/COCO_Format/coco_val_data.json\n","\u001b[32m[03/27 17:11:46 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n","\u001b[32m[03/27 17:11:46 d2.data.common]: \u001b[0mSerializing 97 elements to byte tensors and concatenating them all ...\n","\u001b[32m[03/27 17:11:46 d2.data.common]: \u001b[0mSerialized dataset takes 0.55 MiB\n","\u001b[32m[03/27 17:11:46 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n","\n","model_final_f10217.pkl: 0.00B [00:00, ?B/s]\u001b[A\n","model_final_f10217.pkl:   0% 8.19k/178M [00:00<1:01:45, 48.0kB/s]\u001b[A\n","model_final_f10217.pkl:   1% 1.95M/178M [00:00<00:21, 8.21MB/s]  \u001b[A\n","model_final_f10217.pkl:   3% 4.78M/178M [00:00<00:10, 15.8MB/s]\u001b[A\n","model_final_f10217.pkl:   5% 8.11M/178M [00:00<00:08, 20.6MB/s]\u001b[A\n","model_final_f10217.pkl:   7% 12.3M/178M [00:00<00:06, 27.3MB/s]\u001b[A\n","model_final_f10217.pkl:  10% 17.5M/178M [00:00<00:04, 35.1MB/s]\u001b[A\n","model_final_f10217.pkl:  13% 23.4M/178M [00:00<00:03, 40.9MB/s]\u001b[A\n","model_final_f10217.pkl:  17% 30.5M/178M [00:00<00:02, 49.7MB/s]\u001b[A\n","model_final_f10217.pkl:  22% 39.6M/178M [00:01<00:02, 62.2MB/s]\u001b[A\n","model_final_f10217.pkl:  29% 50.9M/178M [00:01<00:01, 77.3MB/s]\u001b[A\n","model_final_f10217.pkl:  36% 64.3M/178M [00:01<00:01, 94.2MB/s]\u001b[A\n","model_final_f10217.pkl:  45% 80.5M/178M [00:01<00:00, 114MB/s] \u001b[A\n","model_final_f10217.pkl:  56% 99.8M/178M [00:01<00:00, 138MB/s]\u001b[A\n","model_final_f10217.pkl:  67% 119M/178M [00:01<00:00, 154MB/s] \u001b[A\n","model_final_f10217.pkl:  80% 142M/178M [00:01<00:00, 176MB/s]\u001b[A\n","model_final_f10217.pkl: 178MB [00:01, 98.9MB/s]              \n","Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n","Some model parameters or buffers are not found in the checkpoint:\n","\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n","\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n","\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n","\u001b[32m[03/27 17:11:48 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n","\u001b[32m[03/27 17:12:11 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 9  total_loss: 2.4  loss_cls: 0.6728  loss_box_reg: 0.01681  loss_mask: 0.6905  loss_rpn_cls: 0.9763  loss_rpn_loc: 0.05138  validation_loss: 1.553    time: 1.0518  last_time: 1.3127  data_time: 0.2238  last_data_time: 0.2113   lr: 0.00018002  max_mem: 6349M\n","\u001b[32m[03/27 17:12:11 d2.engine.hooks]: \u001b[0mOverall training speed: 8 iterations in 0:00:08 (1.0518 s / it)\n","\u001b[32m[03/27 17:12:11 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:19 (0:00:10 on hooks)\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:12:11 d2.data.datasets.coco]: \u001b[0m\n","Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n","\n","\u001b[32m[03/27 17:12:11 d2.data.datasets.coco]: \u001b[0mLoaded 97 images in COCO format from /content/drive/MyDrive/Thesis_Organized/Data/COCO_Format/coco_val_data.json\n","\u001b[32m[03/27 17:12:11 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n","\u001b[32m[03/27 17:12:11 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n","\u001b[32m[03/27 17:12:11 d2.data.common]: \u001b[0mSerializing 97 elements to byte tensors and concatenating them all ...\n","\u001b[32m[03/27 17:12:11 d2.data.common]: \u001b[0mSerialized dataset takes 0.55 MiB\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:12:11 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n","  2% 2/100 [01:02<49:17, 30.18s/it]dict_values(['COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml', 'diou', 'smooth_l1', 'ciou', 0.9, 0.00016])\n","\u001b[32m[03/27 17:12:13 d2.engine.defaults]: \u001b[0mModel:\n","GeneralizedRCNN(\n","  (backbone): FPN(\n","    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (top_block): LastLevelMaxPool()\n","    (bottom_up): ResNet(\n","      (stem): BasicStem(\n","        (conv1): Conv2d(\n","          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","      )\n","      (res2): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res3): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res4): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (4): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (5): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (6): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (7): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (8): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (9): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (10): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (11): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (12): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (13): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (14): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (15): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (16): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (17): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (18): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (19): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (20): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (21): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (22): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res5): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (proposal_generator): RPN(\n","    (rpn_head): StandardRPNHead(\n","      (conv): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (anchor_generator): DefaultAnchorGenerator(\n","      (cell_anchors): BufferList()\n","    )\n","  )\n","  (roi_heads): StandardROIHeads(\n","    (box_pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n","        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n","        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (box_head): FastRCNNConvFCHead(\n","      (flatten): Flatten(start_dim=1, end_dim=-1)\n","      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc_relu1): ReLU()\n","      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n","      (fc_relu2): ReLU()\n","    )\n","    (box_predictor): FastRCNNOutputLayers(\n","      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n","    )\n","    (mask_pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n","        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n","        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (mask_head): MaskRCNNConvUpsampleHead(\n","      (mask_fcn1): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn2): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn3): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn4): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n","      (deconv_relu): ReLU()\n","      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n",")\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:12:14 d2.data.datasets.coco]: \u001b[0m\n","Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n","\n","\u001b[32m[03/27 17:12:14 d2.data.datasets.coco]: \u001b[0mLoaded 791 images in COCO format from /content/drive/MyDrive/Thesis_Organized/Data/COCO_Format/coco_train_data.json\n","\u001b[32m[03/27 17:12:14 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 791 images left.\n","\u001b[32m[03/27 17:12:14 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n","\u001b[32m[03/27 17:12:14 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n","\u001b[32m[03/27 17:12:14 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n","\u001b[32m[03/27 17:12:14 d2.data.common]: \u001b[0mSerializing 791 elements to byte tensors and concatenating them all ...\n","\u001b[32m[03/27 17:12:14 d2.data.common]: \u001b[0mSerialized dataset takes 4.34 MiB\n","\u001b[32m[03/27 17:12:14 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=4\n","\u001b[32m[03/27 17:12:14 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:12:14 d2.data.datasets.coco]: \u001b[0m\n","Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n","\n","\u001b[32m[03/27 17:12:14 d2.data.datasets.coco]: \u001b[0mLoaded 97 images in COCO format from /content/drive/MyDrive/Thesis_Organized/Data/COCO_Format/coco_val_data.json\n","\u001b[32m[03/27 17:12:14 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n","\u001b[32m[03/27 17:12:14 d2.data.common]: \u001b[0mSerializing 97 elements to byte tensors and concatenating them all ...\n","\u001b[32m[03/27 17:12:14 d2.data.common]: \u001b[0mSerialized dataset takes 0.55 MiB\n","\u001b[32m[03/27 17:12:14 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x/139653917/model_final_2d9806.pkl ...\n","\n","model_final_2d9806.pkl: 0.00B [00:00, ?B/s]\u001b[A\n","model_final_2d9806.pkl:   0% 41.0k/431M [00:00<17:50, 403kB/s]\u001b[A\n","model_final_2d9806.pkl:   0% 1.82M/431M [00:00<00:43, 9.98MB/s]\u001b[A\n","model_final_2d9806.pkl:   1% 4.69M/431M [00:00<00:25, 16.9MB/s]\u001b[A\n","model_final_2d9806.pkl:   2% 8.34M/431M [00:00<00:18, 22.7MB/s]\u001b[A\n","model_final_2d9806.pkl:   3% 12.9M/431M [00:00<00:13, 30.3MB/s]\u001b[A\n","model_final_2d9806.pkl:   4% 17.7M/431M [00:00<00:12, 34.5MB/s]\u001b[A\n","model_final_2d9806.pkl:   6% 24.2M/431M [00:00<00:09, 43.9MB/s]\u001b[A\n","model_final_2d9806.pkl:   7% 32.0M/431M [00:00<00:07, 54.3MB/s]\u001b[A\n","model_final_2d9806.pkl:   9% 40.9M/431M [00:00<00:06, 63.2MB/s]\u001b[A\n","model_final_2d9806.pkl:  12% 52.2M/431M [00:01<00:04, 78.2MB/s]\u001b[A\n","model_final_2d9806.pkl:  15% 65.9M/431M [00:01<00:03, 95.7MB/s]\u001b[A\n","model_final_2d9806.pkl:  19% 83.1M/431M [00:01<00:02, 118MB/s] \u001b[A\n","model_final_2d9806.pkl:  24% 103M/431M [00:01<00:02, 142MB/s] \u001b[A\n","model_final_2d9806.pkl:  28% 122M/431M [00:01<00:01, 156MB/s]\u001b[A\n","model_final_2d9806.pkl:  33% 143M/431M [00:01<00:01, 174MB/s]\u001b[A\n","model_final_2d9806.pkl:  39% 168M/431M [00:01<00:01, 194MB/s]\u001b[A\n","model_final_2d9806.pkl:  44% 189M/431M [00:01<00:01, 201MB/s]\u001b[A\n","model_final_2d9806.pkl:  49% 211M/431M [00:01<00:01, 205MB/s]\u001b[A\n","model_final_2d9806.pkl:  54% 233M/431M [00:01<00:00, 211MB/s]\u001b[A\n","model_final_2d9806.pkl:  59% 254M/431M [00:02<00:00, 210MB/s]\u001b[A\n","model_final_2d9806.pkl:  64% 276M/431M [00:02<00:00, 214MB/s]\u001b[A\n","model_final_2d9806.pkl:  68% 293M/431M [00:02<00:00, 201MB/s]\u001b[A\n","model_final_2d9806.pkl:  73% 313M/431M [00:02<00:00, 201MB/s]\u001b[A\n","model_final_2d9806.pkl:  79% 339M/431M [00:02<00:00, 219MB/s]\u001b[A\n","model_final_2d9806.pkl:  84% 362M/431M [00:02<00:00, 222MB/s]\u001b[A\n","model_final_2d9806.pkl:  89% 384M/431M [00:02<00:00, 220MB/s]\u001b[A\n","model_final_2d9806.pkl:  93% 400M/431M [00:02<00:00, 201MB/s]\u001b[A\n","model_final_2d9806.pkl:  96% 413M/431M [00:02<00:00, 180MB/s]\u001b[A\n","model_final_2d9806.pkl: 431MB [00:03, 143MB/s]               \n","Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n","Some model parameters or buffers are not found in the checkpoint:\n","\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n","\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n","\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n","\u001b[32m[03/27 17:12:17 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n","\u001b[32m[03/27 17:13:23 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 9  total_loss: 2.724  loss_cls: 0.6413  loss_box_reg: 0.7389  loss_mask: 0.6925  loss_rpn_cls: 0.5916  loss_rpn_loc: 0.05348  validation_loss: 2.311    time: 4.0532  last_time: 4.2230  data_time: 0.1022  last_data_time: 0.0142   lr: 0.00014402  max_mem: 10551M\n","\u001b[32m[03/27 17:13:23 d2.engine.hooks]: \u001b[0mOverall training speed: 8 iterations in 0:00:32 (4.0532 s / it)\n","\u001b[32m[03/27 17:13:23 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:56 (0:00:24 on hooks)\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:13:23 d2.data.datasets.coco]: \u001b[0m\n","Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n","\n","\u001b[32m[03/27 17:13:23 d2.data.datasets.coco]: \u001b[0mLoaded 97 images in COCO format from /content/drive/MyDrive/Thesis_Organized/Data/COCO_Format/coco_val_data.json\n","\u001b[32m[03/27 17:13:23 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n","\u001b[32m[03/27 17:13:23 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n","\u001b[32m[03/27 17:13:23 d2.data.common]: \u001b[0mSerializing 97 elements to byte tensors and concatenating them all ...\n","\u001b[32m[03/27 17:13:23 d2.data.common]: \u001b[0mSerialized dataset takes 0.55 MiB\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:13:23 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n","  3% 3/100 [02:14<1:19:55, 49.44s/it]dict_values(['COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml', 'giou', 'diou', 'ciou', 0.8, 0.0002])\n","\u001b[32m[03/27 17:13:24 d2.engine.defaults]: \u001b[0mModel:\n","GeneralizedRCNN(\n","  (backbone): FPN(\n","    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (top_block): LastLevelMaxPool()\n","    (bottom_up): ResNet(\n","      (stem): BasicStem(\n","        (conv1): Conv2d(\n","          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","      )\n","      (res2): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res3): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res4): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (4): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (5): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res5): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (proposal_generator): RPN(\n","    (rpn_head): StandardRPNHead(\n","      (conv): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (anchor_generator): DefaultAnchorGenerator(\n","      (cell_anchors): BufferList()\n","    )\n","  )\n","  (roi_heads): StandardROIHeads(\n","    (box_pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n","        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n","        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (box_head): FastRCNNConvFCHead(\n","      (flatten): Flatten(start_dim=1, end_dim=-1)\n","      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc_relu1): ReLU()\n","      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n","      (fc_relu2): ReLU()\n","    )\n","    (box_predictor): FastRCNNOutputLayers(\n","      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n","    )\n","    (mask_pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n","        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n","        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (mask_head): MaskRCNNConvUpsampleHead(\n","      (mask_fcn1): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn2): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn3): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn4): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n","      (deconv_relu): ReLU()\n","      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n",")\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:13:25 d2.data.datasets.coco]: \u001b[0m\n","Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n","\n","\u001b[32m[03/27 17:13:25 d2.data.datasets.coco]: \u001b[0mLoaded 791 images in COCO format from /content/drive/MyDrive/Thesis_Organized/Data/COCO_Format/coco_train_data.json\n","\u001b[32m[03/27 17:13:25 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 791 images left.\n","\u001b[32m[03/27 17:13:25 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n","\u001b[32m[03/27 17:13:25 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n","\u001b[32m[03/27 17:13:25 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n","\u001b[32m[03/27 17:13:25 d2.data.common]: \u001b[0mSerializing 791 elements to byte tensors and concatenating them all ...\n","\u001b[32m[03/27 17:13:25 d2.data.common]: \u001b[0mSerialized dataset takes 4.34 MiB\n","\u001b[32m[03/27 17:13:25 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=4\n","\u001b[32m[03/27 17:13:25 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:13:25 d2.data.datasets.coco]: \u001b[0m\n","Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n","\n","\u001b[32m[03/27 17:13:25 d2.data.datasets.coco]: \u001b[0mLoaded 97 images in COCO format from /content/drive/MyDrive/Thesis_Organized/Data/COCO_Format/coco_val_data.json\n","\u001b[32m[03/27 17:13:25 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n","\u001b[32m[03/27 17:13:25 d2.data.common]: \u001b[0mSerializing 97 elements to byte tensors and concatenating them all ...\n","\u001b[32m[03/27 17:13:25 d2.data.common]: \u001b[0mSerialized dataset takes 0.55 MiB\n","\u001b[32m[03/27 17:13:25 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n","Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n","Some model parameters or buffers are not found in the checkpoint:\n","\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n","\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n","\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n","\u001b[32m[03/27 17:13:25 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n","\u001b[32m[03/27 17:13:48 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 9  total_loss: 2.499  loss_cls: 0.5826  loss_box_reg: 0.03063  loss_mask: 0.6924  loss_rpn_cls: 1.094  loss_rpn_loc: 0.05419  validation_loss: 1.711    time: 0.9683  last_time: 1.0275  data_time: 0.1472  last_data_time: 0.0240   lr: 0.00018002  max_mem: 10551M\n","\u001b[32m[03/27 17:13:48 d2.engine.hooks]: \u001b[0mOverall training speed: 8 iterations in 0:00:07 (0.9684 s / it)\n","\u001b[32m[03/27 17:13:48 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:19 (0:00:12 on hooks)\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:13:48 d2.data.datasets.coco]: \u001b[0m\n","Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n","\n","\u001b[32m[03/27 17:13:48 d2.data.datasets.coco]: \u001b[0mLoaded 97 images in COCO format from /content/drive/MyDrive/Thesis_Organized/Data/COCO_Format/coco_val_data.json\n","\u001b[32m[03/27 17:13:48 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n","\u001b[32m[03/27 17:13:48 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n","\u001b[32m[03/27 17:13:48 d2.data.common]: \u001b[0mSerializing 97 elements to byte tensors and concatenating them all ...\n","\u001b[32m[03/27 17:13:48 d2.data.common]: \u001b[0mSerialized dataset takes 0.55 MiB\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:13:48 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n","  4% 4/100 [02:39<1:03:34, 39.74s/it]dict_values(['COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml', 'ciou', 'diou', 'smooth_l1', 0.95, 0.0001])\n","\u001b[32m[03/27 17:13:50 d2.engine.defaults]: \u001b[0mModel:\n","GeneralizedRCNN(\n","  (backbone): FPN(\n","    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (top_block): LastLevelMaxPool()\n","    (bottom_up): ResNet(\n","      (stem): BasicStem(\n","        (conv1): Conv2d(\n","          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","      )\n","      (res2): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res3): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res4): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (4): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (5): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res5): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (proposal_generator): RPN(\n","    (rpn_head): StandardRPNHead(\n","      (conv): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (anchor_generator): DefaultAnchorGenerator(\n","      (cell_anchors): BufferList()\n","    )\n","  )\n","  (roi_heads): StandardROIHeads(\n","    (box_pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n","        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n","        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (box_head): FastRCNNConvFCHead(\n","      (flatten): Flatten(start_dim=1, end_dim=-1)\n","      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc_relu1): ReLU()\n","      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n","      (fc_relu2): ReLU()\n","    )\n","    (box_predictor): FastRCNNOutputLayers(\n","      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n","    )\n","    (mask_pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n","        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n","        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (mask_head): MaskRCNNConvUpsampleHead(\n","      (mask_fcn1): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn2): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn3): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn4): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n","      (deconv_relu): ReLU()\n","      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n",")\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:13:50 d2.data.datasets.coco]: \u001b[0m\n","Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n","\n","\u001b[32m[03/27 17:13:50 d2.data.datasets.coco]: \u001b[0mLoaded 791 images in COCO format from /content/drive/MyDrive/Thesis_Organized/Data/COCO_Format/coco_train_data.json\n","\u001b[32m[03/27 17:13:50 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 791 images left.\n","\u001b[32m[03/27 17:13:50 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n","\u001b[32m[03/27 17:13:50 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n","\u001b[32m[03/27 17:13:50 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n","\u001b[32m[03/27 17:13:50 d2.data.common]: \u001b[0mSerializing 791 elements to byte tensors and concatenating them all ...\n","\u001b[32m[03/27 17:13:50 d2.data.common]: \u001b[0mSerialized dataset takes 4.34 MiB\n","\u001b[32m[03/27 17:13:50 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=4\n","\u001b[32m[03/27 17:13:50 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:13:50 d2.data.datasets.coco]: \u001b[0m\n","Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n","\n","\u001b[32m[03/27 17:13:50 d2.data.datasets.coco]: \u001b[0mLoaded 97 images in COCO format from /content/drive/MyDrive/Thesis_Organized/Data/COCO_Format/coco_val_data.json\n","\u001b[32m[03/27 17:13:50 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n","\u001b[32m[03/27 17:13:50 d2.data.common]: \u001b[0mSerializing 97 elements to byte tensors and concatenating them all ...\n","\u001b[32m[03/27 17:13:50 d2.data.common]: \u001b[0mSerialized dataset takes 0.55 MiB\n","\u001b[32m[03/27 17:13:50 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n","Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n","Some model parameters or buffers are not found in the checkpoint:\n","\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n","\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n","\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n","\u001b[32m[03/27 17:13:50 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n","\u001b[32m[03/27 17:14:13 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 9  total_loss: 2.406  loss_cls: 0.5677  loss_box_reg: 0.02469  loss_mask: 0.6919  loss_rpn_cls: 1.069  loss_rpn_loc: 0.0773  validation_loss: 1.951    time: 0.9930  last_time: 0.9970  data_time: 0.1923  last_data_time: 0.0137   lr: 9.001e-05  max_mem: 10551M\n","\u001b[32m[03/27 17:14:13 d2.engine.hooks]: \u001b[0mOverall training speed: 8 iterations in 0:00:07 (0.9930 s / it)\n","\u001b[32m[03/27 17:14:13 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:19 (0:00:11 on hooks)\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:14:13 d2.data.datasets.coco]: \u001b[0m\n","Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n","\n","\u001b[32m[03/27 17:14:13 d2.data.datasets.coco]: \u001b[0mLoaded 97 images in COCO format from /content/drive/MyDrive/Thesis_Organized/Data/COCO_Format/coco_val_data.json\n","\u001b[32m[03/27 17:14:13 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n","\u001b[32m[03/27 17:14:13 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n","\u001b[32m[03/27 17:14:13 d2.data.common]: \u001b[0mSerializing 97 elements to byte tensors and concatenating them all ...\n","\u001b[32m[03/27 17:14:13 d2.data.common]: \u001b[0mSerialized dataset takes 0.55 MiB\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:14:13 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n","  5% 5/100 [03:04<54:22, 34.34s/it]  dict_values(['COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml', 'ciou', 'smooth_l1', 'diou', 0.95, 0.00016])\n","\u001b[32m[03/27 17:14:14 d2.engine.defaults]: \u001b[0mModel:\n","GeneralizedRCNN(\n","  (backbone): FPN(\n","    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (top_block): LastLevelMaxPool()\n","    (bottom_up): ResNet(\n","      (stem): BasicStem(\n","        (conv1): Conv2d(\n","          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","      )\n","      (res2): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res3): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res4): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (4): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (5): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (6): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (7): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (8): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (9): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (10): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (11): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (12): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (13): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (14): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (15): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (16): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (17): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (18): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (19): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (20): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (21): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (22): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res5): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (proposal_generator): RPN(\n","    (rpn_head): StandardRPNHead(\n","      (conv): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (anchor_generator): DefaultAnchorGenerator(\n","      (cell_anchors): BufferList()\n","    )\n","  )\n","  (roi_heads): StandardROIHeads(\n","    (box_pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n","        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n","        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (box_head): FastRCNNConvFCHead(\n","      (flatten): Flatten(start_dim=1, end_dim=-1)\n","      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc_relu1): ReLU()\n","      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n","      (fc_relu2): ReLU()\n","    )\n","    (box_predictor): FastRCNNOutputLayers(\n","      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n","    )\n","    (mask_pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n","        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n","        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (mask_head): MaskRCNNConvUpsampleHead(\n","      (mask_fcn1): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn2): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn3): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn4): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n","      (deconv_relu): ReLU()\n","      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n",")\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:14:14 d2.data.datasets.coco]: \u001b[0m\n","Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n","\n","\u001b[32m[03/27 17:14:14 d2.data.datasets.coco]: \u001b[0mLoaded 791 images in COCO format from /content/drive/MyDrive/Thesis_Organized/Data/COCO_Format/coco_train_data.json\n","\u001b[32m[03/27 17:14:15 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 791 images left.\n","\u001b[32m[03/27 17:14:15 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n","\u001b[32m[03/27 17:14:15 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n","\u001b[32m[03/27 17:14:15 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n","\u001b[32m[03/27 17:14:15 d2.data.common]: \u001b[0mSerializing 791 elements to byte tensors and concatenating them all ...\n","\u001b[32m[03/27 17:14:15 d2.data.common]: \u001b[0mSerialized dataset takes 4.34 MiB\n","\u001b[32m[03/27 17:14:15 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=4\n","\u001b[32m[03/27 17:14:15 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:14:15 d2.data.datasets.coco]: \u001b[0m\n","Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n","\n","\u001b[32m[03/27 17:14:15 d2.data.datasets.coco]: \u001b[0mLoaded 97 images in COCO format from /content/drive/MyDrive/Thesis_Organized/Data/COCO_Format/coco_val_data.json\n","\u001b[32m[03/27 17:14:15 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n","\u001b[32m[03/27 17:14:15 d2.data.common]: \u001b[0mSerializing 97 elements to byte tensors and concatenating them all ...\n","\u001b[32m[03/27 17:14:15 d2.data.common]: \u001b[0mSerialized dataset takes 0.55 MiB\n","\u001b[32m[03/27 17:14:15 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x/138205316/model_final_a3ec72.pkl ...\n","Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n","Some model parameters or buffers are not found in the checkpoint:\n","\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n","\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n","\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n","\u001b[32m[03/27 17:14:15 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n","Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4bd170ef80>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1443, in _shutdown_workers\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 149, in join\n","    res = self._popen.wait(timeout)\n","  File \"/usr/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n","    if not wait([self.sentinel], timeout):\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n","    ready = selector.select(timeout)\n","  File \"/usr/lib/python3.10/selectors.py\", line 416, in select\n","    fd_event_list = self._selector.poll(timeout)\n","KeyboardInterrupt: \n","\u001b[32m[03/27 17:14:35 d2.engine.hooks]: \u001b[0mOverall training speed: 7 iterations in 0:00:10 (1.5199 s / it)\n","\u001b[32m[03/27 17:14:35 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:15 (0:00:04 on hooks)\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:14:35 d2.data.datasets.coco]: \u001b[0m\n","Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n","\n","\u001b[32m[03/27 17:14:35 d2.data.datasets.coco]: \u001b[0mLoaded 97 images in COCO format from /content/drive/MyDrive/Thesis_Organized/Data/COCO_Format/coco_val_data.json\n","\u001b[32m[03/27 17:14:35 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n","\u001b[32m[03/27 17:14:35 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n","\u001b[32m[03/27 17:14:35 d2.data.common]: \u001b[0mSerializing 97 elements to byte tensors and concatenating them all ...\n","\u001b[32m[03/27 17:14:35 d2.data.common]: \u001b[0mSerialized dataset takes 0.55 MiB\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/27 17:14:35 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n","\u001b[32m[03/27 17:14:35 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 9  total_loss: 2.527  loss_cls: 0.6276  loss_box_reg: 0.436  loss_mask: 0.6931  loss_rpn_cls: 0.6282  loss_rpn_loc: 0.03398    time: 1.3299  last_time: 1.2476  data_time: 0.2593  last_data_time: 0.0139   lr: 0.00014402  max_mem: 10551M\n","  5% 5/100 [03:26<1:05:24, 41.31s/it]\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/Thesis_Organized/Detectron2/train_detectron2.py\", line 146, in <module>\n","    random_search()\n","  File \"/content/drive/MyDrive/Thesis_Organized/Detectron2/train_detectron2.py\", line 118, in random_search\n","    train(backbone, rpn_loss, roi_loss, retinanet_loss, momentum, lr)\n","  File \"/content/drive/MyDrive/Thesis_Organized/Detectron2/train_detectron2.py\", line 87, in train\n","    trainer.train()\n","  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/defaults.py\", line 486, in train\n","    super().train(self.start_iter, self.max_iter)\n","  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/train_loop.py\", line 156, in train\n","    self.after_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/train_loop.py\", line 190, in after_step\n","    h.after_step()\n","  File \"/content/drive/MyDrive/Thesis_Organized/Detectron2/customtrainer.py\", line 113, in after_step\n","    self._do_loss_eval()\n","  File \"/content/drive/MyDrive/Thesis_Organized/Detectron2/customtrainer.py\", line 80, in _do_loss_eval\n","    loss_batch = self._get_loss(inputs)  # Compute loss for the current batch\n","  File \"/content/drive/MyDrive/Thesis_Organized/Detectron2/customtrainer.py\", line 100, in _get_loss\n","    metrics_dict = self._model(data)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/detectron2/modeling/meta_arch/rcnn.py\", line 158, in forward\n","    features = self.backbone(images.tensor)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/detectron2/modeling/backbone/fpn.py\", line 139, in forward\n","    bottom_up_features = self.bottom_up(x)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/detectron2/modeling/backbone/resnet.py\", line 449, in forward\n","    x = stage(x)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n","    input = module(input)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/detectron2/modeling/backbone/resnet.py\", line 201, in forward\n","    out = self.conv3(out)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/detectron2/layers/wrappers.py\", line 127, in forward\n","    x = F.conv2d(\n","KeyboardInterrupt\n","^C\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"wWQDqWNwqo8M"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}